<!doctype html><html lang=en-us><head><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Choosing Prior Upvote Probability We currently use a prior of $Beta(0.25, .025)$ for the uninformed upvote probability.
This was arrived at from two different directions:
1. The Empirical Prior The &lsquo;Empirical Prior&rsquo; of $Beta(."><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.131.0"><title>- Social Protocols</title>
<meta property="og:title" content=" - Social Protocols"><link rel=stylesheet href=/css/fonts.css media=all><link rel=stylesheet href=/css/main.css media=all><script data-goatcounter=https://socialprotocols.goatcounter.com/count async src=//gc.zgo.at/count.js></script></head><body><div class=wrapper><header class=header><nav class=nav><a href=/ class=nav-logo style=margin-right:10px><img src=/images/logo.svg width=50 height=50 alt=Logo>
</a><a href=/ class=nav-title>Social Protocols</a><ul class=nav-links><li><a href=/articles>Articles</a></li><li><a href=/#projects>Projects</a></li><li><a href=/research-notes>Research Notes</a></li><li><a href=/resources>Resources</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title></h1><div class=article-content><h1 id=choosing-prior-upvote-probability>Choosing Prior Upvote Probability</h1><p>We currently use a prior of $<code>Beta(0.25, .025)</code>$ for the uninformed upvote probability.</p><p>This was arrived at from two different directions:</p><h2 id=1-the-empirical-prior>1. The Empirical Prior</h2><p>The &lsquo;Empirical Prior&rsquo; of $<code>Beta(.27, .27)</code>$ is from <a href=https://www.researchgate.net/publication/340042592_The_Bayesian_sampler_Generic_Bayesian_inference_causes_incoherence_in_human_probability_judgments>Zhu, Jian-Qiao & Sanborn, Adam & Chater, Nick. (2020)</a><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, based on an attempt to answer the question &ldquo;What is the most appropriate generic prior Beta distribution&mldr;&rdquo;. The idea is that a Uniform prior $<code>Beta(1,1)</code>$ may not reflect the real world. In the real world, we tend to deal with claims that are probably true or probably false. The Haldane&rsquo;s Prior $<code>Beta(0,0)</code>$ and Jeffrey&rsquo;s Prior $<code>Beta(0.5,0.5)</code>$ are two Beta distributions that have this property. But the authors used an analysis of human language to come up with an Empirical prior of $<code>Beta(.27,.27)</code>$.</p><h2 id=2-our-intuitions>2. Our Intuitions</h2><p>We have some intuitions of how scoring should work when there are few votes:</p><ol><li>initially, when there is only the submitter&rsquo;s upvote, p should be close to .85 based on anecdotal data on upvote/downvote ratios from Reddit</li><li>if there is an equal number of upvotes and downvotes, there doesn&rsquo;t seem reason for p to be anything other than 50%</li><li>if a user posts a counter-argument with their vote, there should be a slight bias in favor of that user until other users vote on or reply</li></ol><p>The expected results of the simulation test two-users-disagreeing.jl reflects these constraints. Only a prior close to $<code>Beta(.27, .27)</code>$ &ndash; the Empirical Prior above &ndash; makes these test pass.</p><p>All the distributions we&rsquo;ve discussed have a mean of 0.5 and a sample &lt;= 1. Focusing on sample sizes, we have a pattern:</p><ul><li>Uniform: 2</li><li>Jeffrey&rsquo;s: 1</li><li>Empirical: .54</li><li>Haldane: 0</li></ul><p>This may all just be a coincidence, but it makes me think that the Empirical prior sample size should be .5 exactly and that it reflects some fundamental distribution. So that&rsquo;s what we&rsquo;re going with for now.</p><h3 id=notes-from-zulip-chat-2024-06-03>Notes from Zulip Chat 2024-06-03</h3><p>From Zulip chat.</p><blockquote><p>Johannes Nakayama
5:24 PM
Interesting! Makes a lot of sense, thinking about it this way. It&rsquo;s probably a better prior for our purposes because the uniform prior doesn&rsquo;t apply so well to a very polarized discourse ecosystem. Might be a better assumption to have a prior belief that reflects that people will either agree or disagree (somewhat strongly) with any given post.</p></blockquote><h2 id=estimating-via-mcmc-simulation>Estimating via MCMC Simulation</h2><p>I previously assumed we&rsquo;d have to discover this global prior empirically using a MCMC simulation once we got enough data. But now I think we have a good guess what the results of that simulation will be. In fact I think the MCMC simulation would necessarily produce similar numbers as long as the data has the following properties:</p><ol><li>the average upvote probability on a post (given the original poster&rsquo;s upvote) is close to .85</li><li>the average upvote probability given the first vote is a downvote (e.g. when there is 1 upvote and 1 downvote) is close to .50</li></ol><p>If empirically, we find these numbers are different, then the global prior upvote probability would have to be whatever made those numbers fit. For example, suppose we find the average upvote probability on a post is .75, and that given the first vote is a downvote, the average upvote probability is .6. We&rsquo;d need a prior $<code>Beta(alpha, beta)</code>$ such that the mean of $<code>(Beta(alpha+1, beta)</code>$ is .75 and the mean of $<code>(Beta(alpha+1, beta+1)</code> is .6.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://www.researchgate.net/publication/340042592_The_Bayesian_sampler_Generic_Bayesian_inference_causes_incoherence_in_human_probability_judgments>Zhu, Jian-Qiao & Sanborn, Adam & Chater, Nick. (2020). The Bayesian Sampler: Generic Bayesian Inference Causes Incoherence in Human Probability Judgments. Psychological Review. 127. 10.1037/rev0000190.</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article></main><footer class=footer><ul class=footer-links><li><a href type=application/rss+xml target=_blank>RSS</a></li><li><a href=https://github.com/social-protocols>GitHub</a></li><li><a href=https://mas.to/@SocialProtocols>Mastodon</a></li><li><a href=https://medium.com/@socialprotocols>Medium</a></li><li><a href=https://socialprotocols.substack.com>Substack</a></li><li><a href=https://twitter.com/socialprotocols>Twitter</a></li><li><a href=https://social-protocols.zulipchat.com/join/3awvls77dbmolwlradnfmkig/>Zulip</a></li></ul></footer></div><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] },
    TeX: { extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js", "noUndefined.js"] },
    messageStyle: "none"
  });
</script><script src=/js/math-code.js></script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>