<!doctype html><html lang=en-us><head><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Measuring Informed Probability Our scoring formula starts with an estimate of the probability that a user upvotes a post, and then observes how exposure to information in the note influences that probability."><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.131.0"><title>- Social Protocols</title>
<meta property="og:title" content=" - Social Protocols"><link rel=stylesheet href=/css/fonts.css media=all><link rel=stylesheet href=/css/main.css media=all><script data-goatcounter=https://socialprotocols.goatcounter.com/count async src=//gc.zgo.at/count.js></script></head><body><div class=wrapper><header class=header><nav class=nav><a href=/ class=nav-logo style=margin-right:10px><img src=/images/logo.svg width=50 height=50 alt=Logo>
</a><a href=/ class=nav-title>Social Protocols</a><ul class=nav-links><li><a href=/articles>Articles</a></li><li><a href=/#projects>Projects</a></li><li><a href=/research-notes>Research Notes</a></li><li><a href=/resources>Resources</a></li></ul></nav></header><main class=content role=main><article class=article><h1 class=article-title></h1><div class=article-content><h1 id=measuring-informed-probability>Measuring Informed Probability</h1><p>Our scoring formula starts with an estimate of the probability that a user upvotes a post, and then observes how exposure to information in the note influences that probability. We differentiate &ldquo;informed&rdquo; vs &ldquo;uninformed&rdquo; upvote probabilities depending on whether users have or have not been &ldquo;exposed&rdquo; to the information.</p><p>But what exactly does it mean to be exposed to the information in the note? Does this mean they have <strong>read</strong> the note? But what if they read it and don&rsquo;t really pay attention to it?</p><p>We can&rsquo;t tell if a user really read the post, let alone really <strong>considered</strong> it. We assume that if we showed the note below the post when they voted on the post, it is more likely that they considered it. And if they voted on the note, they almost certainly considered it.</p><p>We can also assume that the proportion of users who consider the note when it is shown to them is the same for all posts, and similarly the proportion of users who vote on the note after considering it is the same for all posts.</p><h2 id=the-causal-model>The Causal Model</h2><p>So that gives us a causal model that looks like this, and some assumptions:</p><ul><li>$N$: note is created</li><li>$T$: user is shown the note</li><li>$C$: user considers the note</li><li>$V$: user votes on the note</li><li>$U$: user upvotes the post</li></ul><pre tabindex=0><code>          N
          ↓
          T
          ↓   
          C
        ↙   ↘  
      U       V
</code></pre><p>With the following assumptions:</p><ul><li>We cannot observe $C$</li><li>$T$ is entirely under our control and can be manipulated randomly.</li><li>$P(V=1|C=0) = 0$ and $P(V=1|C=1)$ is a global constant.</li><li>Assume $P(C=1|T=0)$ is nonzero, and that $T=1$ increases the probability of $C=1$ by some constant, and $P(C=1|T=1)$ is the same for all posts.</li></ul><p>So what do we want to ask of this model? Well, what we really want is to estimate the effect of <strong>considering</strong> the note on the probability of upvoting the post.</p><p>Let&rsquo;s define the Informed Upvote Probability as:</p><p>$$
P(U=1|do(C=1))
$$</p><p>Since the only arrow into $U$ is from $C$, this is equal to:</p><p>$$
P(U=1|C=1)
$$</p><p>The informed probability is the <em>maximum</em> possible effect of the information. Any intervention we do to bring the note to the user&rsquo;s attention can&rsquo;t have an effect beyond the user properly considering the information.</p><h2 id=v-as-a-proxy-for-c>V as a proxy for C</h2><p>Given our assumptions above, we can treat $V$ as a proxy for the unobserved variable $C$. Since $P(V=1|C=1)$ is a constant and $P(V=1|C=0)=0$, the users who vote on the note are a representative sample of those who consider it. So we can estimate:</p><p>$$
P(U|C=1) \approx P(U|V=1) =
\frac{\text{Number of upvotes on posts when note is voted on}}{\text{Total number of votes on posts when note is voted on}}
$$</p><h2 id=shown-vs-voted-on-note>Shown vs Voted on Note</h2><p>When a user votes on the note, we know they considered it. If they were shown the note, but did not upvote, there is less of a chance they considered it. Thus, the effect of showing the note should be less than the full effect of considering the note, but in the same direction.</p><p>So the events $T$ and $V$ increase the probability of $U$ in proportion to the amount they increase the probability of $C$. This relationship is illustrated in the chart below.</p><pre tabindex=0><code>                 |                 |
                 |                 ×-P(U|C=1) = P(U|V=1)
                 |              ∙  |
                 |           × ------P(U|T=1,V=0)
                 |        ∙  |     |
    P(U|T=0,V=0)------ ×     |     |
                 |  ∙  |     |     |
        P(U|C=0)-∙     |     |     |
                 |     |     |     | 
                  -----|-----|-----|
                       |     |     |
               P(C|T=0,V=0)  | P(C|V=1)=1
                             | 
                         P(C|T=1,V=0)
</code></pre><p>The slope of the line is $R(U,C) = P(U|C=1) - P(U|C=0)$. I call this slope the <strong>relevance</strong> of $C$ to $U$, as defined in my essays on <a href=https://jonathanwarden.com/bayesian-argumentation/>Bayesian Argumentation</a>.</p><p>So before being shown the note, the probability of considering the note is $P(C|T=0,V=0)$, and the probability of upvoting the post is $P(U|T=0,V=0)$. This is the first x on the line.</p><p>This value is greater than $P(U|C=0)$ because even a user who wasn&rsquo;t shown the note under the post when they voted on it may have found it and considered it some other way.</p><p>If the user is shown the note, but has not yet voted on it, then the probability that they considered the note moves to $P(C|T=1,V=0)$, and consequently the probability of upvoting the note moves to $P(U|T=1,V=0)$. This is the second × on the line.</p><p>Finally, when the user votes on the note, the probability that they considered the note moves to 1, and consequently the probability of upvoting the note moves to $P(U|V=1) = P(U|C=1)$. This is the third × on the line.</p><h2 id=extrapolating>Extrapolating</h2><p>Once we find any two points on this line, we can extrapolate any other point. This is great:</p><ul><li>If we know $P(U|T=0,V=0)$ and $P(U|T=1,V=0)$ we can extrapolate $P(U|V=1)=P(U|C=1)$. This is useful if we have sparse data (nobody has voted on the note).</li><li>If we know $P(U|T=0,V=0)$ and $P(U|V=1)$ we can extrapolate $P(U|T=1,V=0)$. This is also</li></ul><p>useful if we have sparse data.</p><ul><li>We can also extrapolate based on $P(U|C=0)$ &ndash; which we can do by considering upvotes that happened <em>before</em> the note was created.</li></ul><p>We can extend this method to other events that bring the note to the attention of the user. This allows us to calculate the value of each of these attention events.</p><h3 id=example-extrapolation>Example Extrapolation</h3><p>Say we observe $P(U|V)=P(U|C=1)$, and $P(U|T=1,V=0)$. We want to extrapolate $P(U|C=0)$:</p><p>$$
\begin{aligned}
P(U|T=1) = P(U|C=0,T=1) + P(U|C=1,T=1) \
= P(U|C=0) \cdot P(C=0|T=1) + P(U|C=1) \cdot P(C|T=1)
\end{aligned}
$$</p><p>$$
P(U|C=0) \cdot P(C=0|T=1) = P(U|T=1) - P(U|C=1) \cdot P(C|T=1)
$$</p><p>$$
P(U|C=0) = \frac{ P(U|T=1) - P(U|C=1) \cdot P(C|T=1) }{ P(C=0|T=1) }
$$</p><h3 id=example-extrapolation-2>Example Extrapolation 2</h3><p>Now say we observe $P(U|T=0,V=0)$ and $P(U|T=1,V=0)$. We can also extrapolate $P(U|V=1)$. Derivation <a href=https://chat.openai.com/share/381d2699-f8b1-4079-87cd-f8180d33d785>here</a>.</p><p>$$
P(U|V=1) = P(U|T=0,V=0) + \frac{ P(U|T=1,V=0) - P(U|T=0,V=0) }{ P(C|T=1,V=0) }
$$</p><h2 id=bayesian-hierarchical-model>Bayesian Hierarchical Model</h2><p>Let&rsquo;s define the following Bayesian hierarchical model:</p><p>Assumptions:</p><ul><li>$t_{i,j}$ is whether user $i$ was shown the note on post $j$ but did not vote.</li><li>$c_{i,j}$ is whether user $i$ considered the note on post $j$.</li><li>$uv_{i,j} = 1$ if user $i$ upvoted post $j$ given they voted on the note.</li><li>$ut_{i,j} = 1$ if user $i$ upvoted post $j$ given they were shown the note.</li><li>$θ_j = P(V=1|V_j=1)$ = informedProbability.</li><li>$θ[0]_j = P(V=1|T_j=0,V_j=0)$ = probabilityGivenNotShown.</li><li>$θ[1]_j = P(V=1|T_j=1,V_j=0)$ = probabilityGivenShown.</li><li>$r = P(V=1|C=1)$ = globalVoteProportion (known constant).</li><li>$s0 = P(C=1|T=0,V=0)$ = globalConsiderationRate (known constant).</li><li>$s1 = P(C=1|T=1,V=0)$ = globalConsiderationRate (known constant).</li><li>$c_{i,j} = s0(1 - t_{i,j}) + s1 \cdot t_{i,j}$</li></ul><p>Since $V$ is a proxy for $C$, then $P(U|V=1) \approx P(U|C=1) = θ$.</p><p>Then we can have beta-bernoulli models to estimate $θ$, $θ[0]$, and $θ[1]$ based on the votes of people who voted on the note.</p><ul><li>$uv_{i,j} = 1$ if user $i$ upvoted post $j$ given they voted on the note.</li></ul><p>$$
θ_j \sim \text{beta}(\text{globalPriorUpvoteProbability})
$$</p><p>$$
uv_{i,j} = \text{bern}(θ_j)
$$</p><p>And another model for $θ[1]$:</p><ul><li>$ut_{i,j} = 1$ if user $i$ upvoted post $j$ given they were shown the note.</li></ul><p>$$
θ[1]_j \sim \text{beta}(\text{globalPriorUpvoteProbability})
$$</p><p>$$
ut_{i,j} = \text{bern}(θ[1]_j)
$$</p><p>And another model for $θ[2]$.</p><h3 id=combining-hierarchical-models-using-pseudo-counts>Combining Hierarchical Models using Pseudo-Counts</h3><p>Now, since the thetas are all related, we can combine these models into one model, where the parameters are slope and intercept, and the $θ$ values are a function of these. But this is super-complicated.</p><p>Another approach would be to estimate $θ$ using the data we have for users that voted on the note. Then we can separately estimate$ θ[1]$ and$ θ[2]$ , and use these to update$ θ$ using a pseudo-tally.</p><p>Suppose we already have $θ=\text{beta}(\alpha,\beta)$ based on the tally of votes of users that voted on the note, and we also have beta distributions for $θ[1]$ and $θ[2]$.</p><p>Then we can take $\text{mean}(θ[1]) \approx P(U|T=1,V=0)$ and $\text{mean}(θ[0]) \approx P(U|T=0,V=0)$ and use this to estimate a value for $θ&rsquo;$. Given</p><p>$$
P(U|V=1) = P(U|T=0,V=0) + \frac{ P(U|T=1,V=0) - P(U|T=0,V=0) }{ P(C|T=1,V=0) }
$$</p><p>So</p><p>$$
θ&rsquo; = \text{mean}(θ[0]) + \frac{ θ[1] - θ[0] }{ s1 }
$$</p><p>We can then create a pseudo-sample-size $n$ by just adding the sample sizes for $θ[1]$ and $θ[2]$ (perhaps multiplying by some constant), and a pseudo count $Z = n \cdot θ&rsquo;$. Then use these to update $θ$.</p><p>So we get a posterior $θ \sim \text{beta}(\alpha + Z, \beta + n - Z)$.</p><h3 id=linear-regression>Linear Regression</h3><p>Another approach would be a linear regression. Doing a Bayesian linear regression would give us a probability distribution for the slope and intercept. From this, we can compute the probability distribution for $θ$, $θ[1]$, or $θ[2]$ values.</p><h2 id=further-thoughts>Further Thoughts</h2><p>We are glossing over some assumptions. We should rewrite the above based on the idea of different events. N -> T -> V. Probability of considering note increases with each event. The event T should be shown note <em>but not voted on it</em>. And the state V should be voted, whether or not it was shown.</p><p>The probability of C increases as each event happens. P(C|V)=1. P(C|N) and P(C|T) should be global constants. We can estimate these by finding the slope R(C,U). If we know the slope and the vertical coordinate of a point on the line (P(U|N) or P(U|T)), we can find the horizontal.</p><h2 id=conclusionssummary>Conclusions/Summary</h2><p>I think that it&rsquo;s best to simplify the model and only look at users who voted on the note. This lets us calculate $θ=P(U|C=1)=P(U|V)$ more directly using a simple beta-bernoulli model.</p></div></article></main><footer class=footer><ul class=footer-links><li><a href type=application/rss+xml target=_blank>RSS</a></li><li><a href=https://github.com/social-protocols>GitHub</a></li><li><a href=https://mas.to/@SocialProtocols>Mastodon</a></li><li><a href=https://medium.com/@socialprotocols>Medium</a></li><li><a href=https://socialprotocols.substack.com>Substack</a></li><li><a href=https://twitter.com/socialprotocols>Twitter</a></li><li><a href=https://social-protocols.zulipchat.com/join/3awvls77dbmolwlradnfmkig/>Zulip</a></li></ul></footer></div><script src=//cdnjs.cloudflare.com/ajax/libs/highlight.js//highlight.min.js></script><script>hljs.configure({languages:[]}),hljs.initHighlightingOnLoad()</script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] },
    TeX: { extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js", "noUndefined.js"] },
    messageStyle: "none"
  });
</script><script src=/js/math-code.js></script><script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>